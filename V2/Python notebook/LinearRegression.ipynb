{"cells":[{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"a568f0a9a5f34683b0b174b7c78f82c6","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"},"source":"# Library","block_group":"a568f0a9a5f34683b0b174b7c78f82c6"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"da356a6b62f84845a240c213f9de43a3","source_hash":null,"execution_start":1677506160237,"execution_millis":7626,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"from library import *\n","block_group":"da356a6b62f84845a240c213f9de43a3","execution_count":null,"outputs":[{"name":"stderr","text":"2023-02-27 13:56:02.779307: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-02-27 13:56:02.943728: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-02-27 13:56:02.943767: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2023-02-27 13:56:02.977598: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-02-27 13:56:04.832141: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2023-02-27 13:56:04.832241: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2023-02-27 13:56:04.832251: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"b130e19a4eb44d74a491193d61689814","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"},"source":"# Data importation","block_group":"b130e19a4eb44d74a491193d61689814"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"a7f2bdf00639476582bfe01ab923b66e","source_hash":null,"execution_start":1677506167886,"execution_millis":106,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"data = pd.read_csv(\"Datasets/data.csv\", sep=\";\")\ndata","block_group":"a7f2bdf00639476582bfe01ab923b66e","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":10,"row_count":8964,"columns":[{"name":"left_char","dtype":"int64","stats":{"unique_count":7,"nan_count":0,"min":"1","max":"7","histogram":[{"bin_start":1,"bin_end":1.6,"count":1263},{"bin_start":1.6,"bin_end":2.2,"count":1221},{"bin_start":2.2,"bin_end":2.8,"count":0},{"bin_start":2.8,"bin_end":3.4,"count":1215},{"bin_start":3.4,"bin_end":4,"count":0},{"bin_start":4,"bin_end":4.6,"count":1194},{"bin_start":4.6,"bin_end":5.2,"count":1332},{"bin_start":5.2,"bin_end":5.8,"count":0},{"bin_start":5.8,"bin_end":6.3999999999999995,"count":1332},{"bin_start":6.3999999999999995,"bin_end":7,"count":1407}]}},{"name":"right_char","dtype":"int64","stats":{"unique_count":7,"nan_count":0,"min":"1","max":"7","histogram":[{"bin_start":1,"bin_end":1.6,"count":1263},{"bin_start":1.6,"bin_end":2.2,"count":1221},{"bin_start":2.2,"bin_end":2.8,"count":0},{"bin_start":2.8,"bin_end":3.4,"count":1215},{"bin_start":3.4,"bin_end":4,"count":0},{"bin_start":4,"bin_end":4.6,"count":1194},{"bin_start":4.6,"bin_end":5.2,"count":1332},{"bin_start":5.2,"bin_end":5.8,"count":0},{"bin_start":5.8,"bin_end":6.3999999999999995,"count":1332},{"bin_start":6.3999999999999995,"bin_end":7,"count":1407}]}},{"name":"strength_left","dtype":"int64","stats":{"unique_count":15,"nan_count":0,"min":"1","max":"15","histogram":[{"bin_start":1,"bin_end":2.4,"count":1911},{"bin_start":2.4,"bin_end":3.8,"count":693},{"bin_start":3.8,"bin_end":5.199999999999999,"count":1617},{"bin_start":5.199999999999999,"bin_end":6.6,"count":759},{"bin_start":6.6,"bin_end":8,"count":639},{"bin_start":8,"bin_end":9.399999999999999,"count":945},{"bin_start":9.399999999999999,"bin_end":10.799999999999999,"count":1608},{"bin_start":10.799999999999999,"bin_end":12.2,"count":423},{"bin_start":12.2,"bin_end":13.6,"count":156},{"bin_start":13.6,"bin_end":15,"count":213}]}},{"name":"strength_right","dtype":"int64","stats":{"unique_count":15,"nan_count":0,"min":"1","max":"15","histogram":[{"bin_start":1,"bin_end":2.4,"count":1911},{"bin_start":2.4,"bin_end":3.8,"count":693},{"bin_start":3.8,"bin_end":5.199999999999999,"count":1617},{"bin_start":5.199999999999999,"bin_end":6.6,"count":759},{"bin_start":6.6,"bin_end":8,"count":639},{"bin_start":8,"bin_end":9.399999999999999,"count":945},{"bin_start":9.399999999999999,"bin_end":10.799999999999999,"count":1608},{"bin_start":10.799999999999999,"bin_end":12.2,"count":423},{"bin_start":12.2,"bin_end":13.6,"count":156},{"bin_start":13.6,"bin_end":15,"count":213}]}},{"name":"value_left","dtype":"int64","stats":{"unique_count":11,"nan_count":0,"min":"0","max":"10","histogram":[{"bin_start":0,"bin_end":1,"count":806},{"bin_start":1,"bin_end":2,"count":424},{"bin_start":2,"bin_end":3,"count":679},{"bin_start":3,"bin_end":4,"count":801},{"bin_start":4,"bin_end":5,"count":673},{"bin_start":5,"bin_end":6,"count":2198},{"bin_start":6,"bin_end":7,"count":673},{"bin_start":7,"bin_end":8,"count":801},{"bin_start":8,"bin_end":9,"count":679},{"bin_start":9,"bin_end":10,"count":1230}]}},{"name":"scenario","dtype":"int64","stats":{"unique_count":9,"nan_count":0,"min":"1","max":"9","histogram":[{"bin_start":1,"bin_end":1.8,"count":996},{"bin_start":1.8,"bin_end":2.6,"count":996},{"bin_start":2.6,"bin_end":3.4000000000000004,"count":996},{"bin_start":3.4000000000000004,"bin_end":4.2,"count":996},{"bin_start":4.2,"bin_end":5,"count":0},{"bin_start":5,"bin_end":5.800000000000001,"count":996},{"bin_start":5.800000000000001,"bin_end":6.6000000000000005,"count":996},{"bin_start":6.6000000000000005,"bin_end":7.4,"count":996},{"bin_start":7.4,"bin_end":8.2,"count":996},{"bin_start":8.2,"bin_end":9,"count":996}]}},{"name":"value_left_rep1","dtype":"int64","stats":{"unique_count":12,"nan_count":0,"min":"-1","max":"10","histogram":[{"bin_start":-1,"bin_end":0.10000000000000009,"count":3526},{"bin_start":0.10000000000000009,"bin_end":1.2000000000000002,"count":270},{"bin_start":1.2000000000000002,"bin_end":2.3000000000000003,"count":460},{"bin_start":2.3000000000000003,"bin_end":3.4000000000000004,"count":532},{"bin_start":3.4000000000000004,"bin_end":4.5,"count":486},{"bin_start":4.5,"bin_end":5.6000000000000005,"count":1404},{"bin_start":5.6000000000000005,"bin_end":6.700000000000001,"count":486},{"bin_start":6.700000000000001,"bin_end":7.800000000000001,"count":532},{"bin_start":7.800000000000001,"bin_end":8.9,"count":460},{"bin_start":8.9,"bin_end":10,"count":808}]}},{"name":"value_left_rep2","dtype":"int64","stats":{"unique_count":12,"nan_count":0,"min":"-1","max":"10","histogram":[{"bin_start":-1,"bin_end":0.10000000000000009,"count":6244},{"bin_start":0.10000000000000009,"bin_end":1.2000000000000002,"count":148},{"bin_start":1.2000000000000002,"bin_end":2.3000000000000003,"count":221},{"bin_start":2.3000000000000003,"bin_end":3.4000000000000004,"count":277},{"bin_start":3.4000000000000004,"bin_end":4.5,"count":203},{"bin_start":4.5,"bin_end":5.6000000000000005,"count":754},{"bin_start":5.6000000000000005,"bin_end":6.700000000000001,"count":203},{"bin_start":6.700000000000001,"bin_end":7.800000000000001,"count":277},{"bin_start":7.800000000000001,"bin_end":8.9,"count":221},{"bin_start":8.9,"bin_end":10,"count":416}]}},{"name":"sex","dtype":"int64","stats":{"unique_count":3,"nan_count":0,"min":"0","max":"2","histogram":[{"bin_start":0,"bin_end":0.2,"count":3888},{"bin_start":0.2,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":1,"count":0},{"bin_start":1,"bin_end":1.2000000000000002,"count":4860},{"bin_start":1.2000000000000002,"bin_end":1.4000000000000001,"count":0},{"bin_start":1.4000000000000001,"bin_end":1.6,"count":0},{"bin_start":1.6,"bin_end":1.8,"count":0},{"bin_start":1.8,"bin_end":2,"count":216}]}},{"name":"age","dtype":"int64","stats":{"unique_count":36,"nan_count":0,"min":"13","max":"80","histogram":[{"bin_start":13,"bin_end":19.7,"count":324},{"bin_start":19.7,"bin_end":26.4,"count":6534},{"bin_start":26.4,"bin_end":33.1,"count":756},{"bin_start":33.1,"bin_end":39.8,"count":108},{"bin_start":39.8,"bin_end":46.5,"count":270},{"bin_start":46.5,"bin_end":53.2,"count":648},{"bin_start":53.2,"bin_end":59.9,"count":270},{"bin_start":59.9,"bin_end":66.6,"count":0},{"bin_start":66.6,"bin_end":73.30000000000001,"count":0},{"bin_start":73.30000000000001,"bin_end":80,"count":54}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"left_char":"2","right_char":"6","strength_left":"7","strength_right":"3","value_left":"8","scenario":"4","value_left_rep1":"-1","value_left_rep2":"-1","sex":"0","age":"57","_deepnote_index_column":"0"},{"left_char":"6","right_char":"5","strength_left":"3","strength_right":"6","value_left":"1","scenario":"2","value_left_rep1":"-1","value_left_rep2":"-1","sex":"0","age":"57","_deepnote_index_column":"1"},{"left_char":"6","right_char":"3","strength_left":"3","strength_right":"8","value_left":"1","scenario":"3","value_left_rep1":"-1","value_left_rep2":"-1","sex":"0","age":"57","_deepnote_index_column":"2"},{"left_char":"7","right_char":"6","strength_left":"10","strength_right":"3","value_left":"0","scenario":"9","value_left_rep1":"-1","value_left_rep2":"-1","sex":"0","age":"57","_deepnote_index_column":"3"},{"left_char":"3","right_char":"5","strength_left":"8","strength_right":"6","value_left":"5","scenario":"7","value_left_rep1":"-1","value_left_rep2":"-1","sex":"0","age":"57","_deepnote_index_column":"4"},{"left_char":"7","right_char":"1","strength_left":"10","strength_right":"1","value_left":"0","scenario":"8","value_left_rep1":"-1","value_left_rep2":"-1","sex":"0","age":"57","_deepnote_index_column":"5"},{"left_char":"1","right_char":"6","strength_left":"1","strength_right":"3","value_left":"5","scenario":"1","value_left_rep1":"-1","value_left_rep2":"-1","sex":"0","age":"57","_deepnote_index_column":"6"},{"left_char":"1","right_char":"6","strength_left":"1","strength_right":"3","value_left":"2","scenario":"5","value_left_rep1":"-1","value_left_rep2":"-1","sex":"0","age":"57","_deepnote_index_column":"7"},{"left_char":"4","right_char":"3","strength_left":"5","strength_right":"8","value_left":"2","scenario":"6","value_left_rep1":"-1","value_left_rep2":"-1","sex":"0","age":"57","_deepnote_index_column":"8"},{"left_char":"1","right_char":"6","strength_left":"1","strength_right":"3","value_left":"2","scenario":"5","value_left_rep1":"2","value_left_rep2":"-1","sex":"0","age":"57","_deepnote_index_column":"9"}]},"text/plain":"      left_char  right_char  strength_left  strength_right  value_left  \\\n0             2           6              7               3           8   \n1             6           5              3               6           1   \n2             6           3              3               8           1   \n3             7           6             10               3           0   \n4             3           5              8               6           5   \n...         ...         ...            ...             ...         ...   \n8959          6           1             11               1           5   \n8960          5           2             14              11           7   \n8961          6           1             11               1           6   \n8962          1           4              1              11           5   \n8963          5           7             14              10           5   \n\n      scenario  value_left_rep1  value_left_rep2  sex  age  \n0            4               -1               -1    0   57  \n1            2               -1               -1    0   57  \n2            3               -1               -1    0   57  \n3            9               -1               -1    0   57  \n4            7               -1               -1    0   57  \n...        ...              ...              ...  ...  ...  \n8959         8                7                8    1   21  \n8960         5                6                7    1   21  \n8961         2                9                5    1   21  \n8962         7                3                2    1   21  \n8963         4                3                5    1   21  \n\n[8964 rows x 10 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>left_char</th>\n      <th>right_char</th>\n      <th>strength_left</th>\n      <th>strength_right</th>\n      <th>value_left</th>\n      <th>scenario</th>\n      <th>value_left_rep1</th>\n      <th>value_left_rep2</th>\n      <th>sex</th>\n      <th>age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>6</td>\n      <td>7</td>\n      <td>3</td>\n      <td>8</td>\n      <td>4</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>57</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>5</td>\n      <td>3</td>\n      <td>6</td>\n      <td>1</td>\n      <td>2</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>57</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6</td>\n      <td>3</td>\n      <td>3</td>\n      <td>8</td>\n      <td>1</td>\n      <td>3</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>57</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7</td>\n      <td>6</td>\n      <td>10</td>\n      <td>3</td>\n      <td>0</td>\n      <td>9</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>57</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>5</td>\n      <td>8</td>\n      <td>6</td>\n      <td>5</td>\n      <td>7</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>57</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8959</th>\n      <td>6</td>\n      <td>1</td>\n      <td>11</td>\n      <td>1</td>\n      <td>5</td>\n      <td>8</td>\n      <td>7</td>\n      <td>8</td>\n      <td>1</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>8960</th>\n      <td>5</td>\n      <td>2</td>\n      <td>14</td>\n      <td>11</td>\n      <td>7</td>\n      <td>5</td>\n      <td>6</td>\n      <td>7</td>\n      <td>1</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>8961</th>\n      <td>6</td>\n      <td>1</td>\n      <td>11</td>\n      <td>1</td>\n      <td>6</td>\n      <td>2</td>\n      <td>9</td>\n      <td>5</td>\n      <td>1</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>8962</th>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>11</td>\n      <td>5</td>\n      <td>7</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>8963</th>\n      <td>5</td>\n      <td>7</td>\n      <td>14</td>\n      <td>10</td>\n      <td>5</td>\n      <td>4</td>\n      <td>3</td>\n      <td>5</td>\n      <td>1</td>\n      <td>21</td>\n    </tr>\n  </tbody>\n</table>\n<p>8964 rows × 10 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"e01e416255004a06908044e25f023934","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"},"source":"# Set train and test dataset","block_group":"e01e416255004a06908044e25f023934"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"df0e4727952f46fcbf6bfd3fe65c32b8","source_hash":null,"execution_start":1677506168030,"execution_millis":0,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#set feature_names\nfeature_names = [\"left_char\",\"right_char\",\"strength_left\",\"strength_right\",\"scenario\",\"value_left_rep1\",\"value_left_rep2\",\"age\",\"sex\"]\n","block_group":"df0e4727952f46fcbf6bfd3fe65c32b8","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"87082a82e98545229f7a3980eb112bd4","source_hash":null,"execution_start":1677506168031,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#get X and Y\nX = data[feature_names]\nY = data.value_left","block_group":"87082a82e98545229f7a3980eb112bd4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"665a46521fa34a42a57459eaf97b2bf4","source_hash":null,"execution_start":1677506168039,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#split in train & test dataset\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)","block_group":"665a46521fa34a42a57459eaf97b2bf4","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"e139e4aa088b47ada43ba33edbe22395","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"},"source":"# Linear Regression","block_group":"e139e4aa088b47ada43ba33edbe22395"},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"493e5c78c88a4a8a89b5efad9aace268","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## Model train","block_group":"493e5c78c88a4a8a89b5efad9aace268"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"0136b41c51724b75a5cc700bf4820c8d","source_hash":null,"execution_start":1677506168518,"execution_millis":6,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"LR = LinearRegression()\n\nparams = {\n    'fit_intercept': [True, False],\n    'normalize': [True, False]\n}\n\ngrid_search_cv = GridSearchCV(LR, params, cv=5, verbose=1)\ngrid_search_cv\n","block_group":"0136b41c51724b75a5cc700bf4820c8d","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"GridSearchCV(cv=5, estimator=LinearRegression(),\n             param_grid={'fit_intercept': [True, False],\n                         'normalize': [True, False]},\n             verbose=1)"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"80f6b63f98f14ef6b9aac8ef981d64a9","source_hash":null,"execution_start":1677138069766,"execution_millis":433,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#model fitting\n#grid_search_cv.fit(X_train, Y_train)\n#best_LR = grid_search_cv.best_estimator_\n#print(\"Best score : \", grid_search_cv.best_score_)\n#print(\"Best: %f using %s\" % (grid_search_cv.best_score_, grid_search_cv.best_params_))\n","block_group":"80f6b63f98f14ef6b9aac8ef981d64a9","execution_count":null,"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 4 candidates, totalling 20 fits\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n  FutureWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n  FutureWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n  FutureWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n  FutureWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n  FutureWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n  FutureWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n  FutureWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n  FutureWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n  FutureWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n  FutureWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n  FutureWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n  FutureWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n  FutureWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n  FutureWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n  FutureWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n  FutureWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n  FutureWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n  FutureWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n  FutureWarning,\nBest score :  0.3452934350946026\nBest: 0.345293 using {'fit_intercept': True, 'normalize': True}\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_base.py:155: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n  FutureWarning,\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n  FutureWarning,\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"266f302619214a3580ce6d272ae75bd1","source_hash":null,"execution_start":1677506173798,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#saving the model\nfilename = 'Model/LR.pkl'\n#print(\"Sauvegarde du modèle dans \", filename)\n#f = open(filename, \"wb\")\n#pickle.dump(best_LR, f)\n#f.close()","block_group":"266f302619214a3580ce6d272ae75bd1","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"895878c05e4e4bc38fa376c233c82be6","source_hash":null,"execution_start":1677506176400,"execution_millis":4,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# loading the model\nLR = pickle.load(open(filename, 'rb'))\nprint(LR)","block_group":"895878c05e4e4bc38fa376c233c82be6","execution_count":null,"outputs":[{"name":"stdout","text":"LinearRegression(normalize=True)\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"960aea8f275944c2a31733cde22bfe9f","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"},"source":"## Model test","block_group":"960aea8f275944c2a31733cde22bfe9f"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"be56edfb6e9d471288e9e238602a2ed2","source_hash":null,"execution_start":1677506179867,"execution_millis":16,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#Test on the test set\nY_test_predict = LR.predict(X_test)\nrmse = (np.sqrt(mean_squared_error(Y_test, Y_test_predict)))\nr2 = r2_score(Y_test, Y_test_predict)\n","block_group":"be56edfb6e9d471288e9e238602a2ed2","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"352d927cf06e4528993f962874bc7343","source_hash":null,"execution_start":1677506182239,"execution_millis":11,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#confusion matrix & report classification\nprint('performance of the model on the test basis')\nprint('--------------------------------------')\nprint('mean square error {}'.format(rmse))\nprint('R2 score {}'.format(r2))\nY_test_predict = Y_test_predict.astype(int)\nprint(\n    classification_report(Y_test,\n                          Y_test_predict,\n                          output_dict=False,\n                          target_names=[str(i) for i in range(max(Y_test)+1)]))","block_group":"352d927cf06e4528993f962874bc7343","execution_count":null,"outputs":[{"name":"stdout","text":"performance of the model on the test basis\n--------------------------------------\nmean square error 2.4378031231356045\nR2 score 0.315275345276271\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00       175\n           1       0.26      0.10      0.14        94\n           2       0.18      0.27      0.22       140\n           3       0.16      0.33      0.22       154\n           4       0.13      0.36      0.19       132\n           5       0.40      0.34      0.37       427\n           6       0.19      0.34      0.25       128\n           7       0.29      0.29      0.29       164\n           8       0.12      0.07      0.09       127\n           9       0.50      0.08      0.14        61\n          10       1.00      0.01      0.02       191\n\n    accuracy                           0.22      1793\n   macro avg       0.29      0.20      0.17      1793\nweighted avg       0.32      0.22      0.20      1793\n\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"cf6ec6fe38fb402a9dcf0b622e5c007a","source_hash":null,"execution_start":1677506186472,"execution_millis":3785,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#confusion matrix of prediction to within 1 class\nY_test = pd.concat([Y_test], ignore_index=True)\nclassification_report_opti(Y_test, Y_test_predict)","block_group":"cf6ec6fe38fb402a9dcf0b622e5c007a","execution_count":null,"outputs":[{"name":"stdout","text":"0 :  {'TN': 1590, 'FN': 164, 'FP': 28, 'TP': 11}\n1 :  {'TN': 1489, 'FN': 52, 'FP': 210, 'TP': 42}\n2 :  {'TN': 1175, 'FN': 53, 'FP': 478, 'TP': 87}\n3 :  {'TN': 863, 'FN': 26, 'FP': 776, 'TP': 128}\n4 :  {'TN': 716, 'FN': 22, 'FP': 945, 'TP': 110}\n5 :  {'TN': 705, 'FN': 119, 'FP': 661, 'TP': 308}\n6 :  {'TN': 978, 'FN': 52, 'FP': 687, 'TP': 76}\n7 :  {'TN': 1249, 'FN': 70, 'FP': 380, 'TP': 94}\n8 :  {'TN': 1465, 'FN': 75, 'FP': 201, 'TP': 52}\n9 :  {'TN': 1664, 'FN': 42, 'FP': 68, 'TP': 19}\n10 :  {'TN': 1594, 'FN': 187, 'FP': 8, 'TP': 4}\nclass | precision | recall | f1-score | accuracy\n0     |  0.282     |  0.063  |  0.103    |  0.893\n1     |  0.167     |  0.447  |  0.243    |  0.854\n2     |  0.154     |  0.621  |  0.247    |  0.704\n3     |  0.142     |  0.831  |  0.243    |  0.553\n4     |  0.104     |  0.833  |  0.185    |  0.461\n5     |  0.318     |  0.721  |  0.441    |  0.565\n6     |  0.1     |  0.594  |  0.171    |  0.588\n7     |  0.198     |  0.573  |  0.294    |  0.749\n8     |  0.206     |  0.409  |  0.274    |  0.846\n9     |  0.218     |  0.311  |  0.256    |  0.939\n10     |  0.333     |  0.021  |  0.04    |  0.891\n\naccuracy :     0.519\n","output_type":"stream"},{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"[0.893, 0.854, 0.704, 0.553, 0.461, 0.565, 0.588, 0.749, 0.846, 0.939, 0.891]"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"04893f18097a4efcbb0542bb521764f0","source_hash":null,"execution_start":1677506193578,"execution_millis":4,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#features importances\ncoef = LR.coef_\ncoef","block_group":"04893f18097a4efcbb0542bb521764f0","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"array([-3.79374297e-02,  3.64104381e-02,  2.27665757e-01, -2.30951387e-01,\n        4.53044274e-05,  2.79384148e-01,  1.69535808e-02,  4.79693134e-04,\n        1.33545806e-02])"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"tags":[],"cell_id":"dc2386fbc16d4664b10291af22ce28f2","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h1"},"source":"# Average accuracy on 10 train/test dataset different","block_group":"dc2386fbc16d4664b10291af22ce28f2"},{"cell_type":"code","metadata":{"tags":[],"cell_id":"48800f8840f64b06a35ca60f84d185ad","source_hash":null,"execution_start":1677507368767,"execution_millis":77989,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"display(LR.get_params())\ncolumn = ['0 acc_0','0 acc_1','1 acc_0','1 acc_1','2 acc_0','2 acc_1','3 acc_0','3 acc_1','4 acc_0','4 acc_1','5 acc_0','5 acc_1','6 acc_0','6 acc_1','7 acc_0','7 acc_1','8 acc_0','8 acc_1','9 acc_0','9 acc_1','10 acc_0','10 acc_1']\nclass_accuracy = pd.DataFrame(columns=column)\nfor i in tqdm(range(10)):\n    print(\" Modèle n°\"+str(i+1)+\" : STARTED \\n\")\n    rand_int = random.randint(2, 1000)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=rand_int)\n\n    #model train\n    LR_test = LinearRegression()\n    LR_test.fit(X_train, Y_train)\n\n    #model test\n    Y_test_predict = LR_test.predict(X_test)\n    rmse = (np.sqrt(mean_squared_error(Y_test, Y_test_predict)))\n    r2 = r2_score(Y_test, Y_test_predict)\n\n    #classification\n    vector = np.vectorize(np.int_)\n    acc_0, prec_0 = accuracy_inclasses(Y_test, vector(Y_test_predict))\n    print('performance of the model on the test basis')\n    print('--------------------------------------')\n    print('mean square error {}'.format(rmse))\n    print('R2 score {}'.format(r2))\n    Y_test_predict = Y_test_predict.astype(int)\n    print(\n    classification_report(Y_test,\n                          Y_test_predict,\n                          output_dict=False,\n                          target_names=[str(i) for i in range(max(Y_test) + 1)]))\n    Y_test = pd.concat([Y_test], ignore_index=True)\n    acc_1 = classification_report_opti(Y_test, Y_test_predict)\n    \n    for j in range(11):\n        class_accuracy.at[i, str(j) + \" acc_0\"] = acc_0[j]\n        class_accuracy.at[i, str(j) + \" acc_1\"] = acc_1[j]\n    print(\"\\n\")\n    print(\" Modèle n°\"+str(i+1)+\" : FINISHED \\n\")\n\n","block_group":"48800f8840f64b06a35ca60f84d185ad","execution_count":null,"outputs":[{"data":{"text/plain":"{'copy_X': True,\n 'fit_intercept': True,\n 'n_jobs': None,\n 'normalize': True,\n 'positive': False}"},"metadata":{},"output_type":"display_data"},{"name":"stderr","text":"0 :  {'TN': 1625, 'FN': 141, 'FP': 24, 'TP': 3}\n1 :  {'TN': 1537, 'FN': 48, 'FP': 165, 'TP': 43}\n2 :  {'TN': 1205, 'FN': 45, 'FP': 455, 'TP': 88}\n3 :  {'TN': 874, 'FN': 25, 'FP': 751, 'TP': 143}\n4 :  {'TN': 696, 'FN': 19, 'FP': 965, 'TP': 113}\n5 :  {'TN': 690, 'FN': 116, 'FP': 667, 'TP': 320}\n6 :  {'TN': 980, 'FN': 49, 'FP': 683, 'TP': 81}\n7 :  {'TN': 1236, 'FN': 70, 'FP': 388, 'TP': 99}\n8 :  {'TN': 1446, 'FN': 87, 'FP': 201, 'TP': 59}\n9 :  {'TN': 1629, 'FN': 56, 'FP': 82, 'TP': 26}\n10 :  {'TN': 1621, 'FN': 152, 'FP': 10, 'TP': 10}\nclass | precision | recall | f1-score | accuracy\n0     |  0.111     |  0.021  |  0.035    |  0.908\n1     |  0.207     |  0.473  |  0.288    |  0.881\n2     |  0.162     |  0.662  |  0.26    |  0.721\n3     |  0.16     |  0.851  |  0.269    |  0.567\n4     |  0.105     |  0.856  |  0.187    |  0.451\n5     |  0.324     |  0.734  |  0.45    |  0.563\n6     |  0.106     |  0.623  |  0.181    |  0.592\n7     |  0.203     |  0.586  |  0.302    |  0.745\n8     |  0.227     |  0.404  |  0.291    |  0.839\n9     |  0.241     |  0.317  |  0.274    |  0.923\n10     |  0.5     |  0.062  |  0.11    |  0.91\n\naccuracy :     0.549\n 20%|██        | 2/10 [00:16<01:04,  8.01s/it]0 :  {'TN': 1625, 'FN': 141, 'FP': 24, 'TP': 3}\n1 :  {'TN': 1537, 'FN': 48, 'FP': 165, 'TP': 43}\n2 :  {'TN': 1205, 'FN': 45, 'FP': 455, 'TP': 88}\n3 :  {'TN': 874, 'FN': 25, 'FP': 751, 'TP': 143}\n4 :  {'TN': 696, 'FN': 19, 'FP': 965, 'TP': 113}\n5 :  {'TN': 690, 'FN': 116, 'FP': 667, 'TP': 320}\n6 :  {'TN': 980, 'FN': 49, 'FP': 683, 'TP': 81}\n7 :  {'TN': 1236, 'FN': 70, 'FP': 388, 'TP': 99}\n8 :  {'TN': 1446, 'FN': 87, 'FP': 201, 'TP': 59}\n9 :  {'TN': 1629, 'FN': 56, 'FP': 82, 'TP': 26}\n10 :  {'TN': 1621, 'FN': 152, 'FP': 10, 'TP': 10}\nclass | precision | recall | f1-score | accuracy\n0     |  0.111     |  0.021  |  0.035    |  0.908\n1     |  0.207     |  0.473  |  0.288    |  0.881\n2     |  0.162     |  0.662  |  0.26    |  0.721\n3     |  0.16     |  0.851  |  0.269    |  0.567\n4     |  0.105     |  0.856  |  0.187    |  0.451\n5     |  0.324     |  0.734  |  0.45    |  0.563\n6     |  0.106     |  0.623  |  0.181    |  0.592\n7     |  0.203     |  0.586  |  0.302    |  0.745\n8     |  0.227     |  0.404  |  0.291    |  0.839\n9     |  0.241     |  0.317  |  0.274    |  0.923\n10     |  0.5     |  0.062  |  0.11    |  0.91\n\naccuracy :     0.549\n\n\n Modèle n°2 : FINISHED \n\n Modèle n°3 : STARTED \n\nclass 0 :  [[1618    1]\n [ 174    0]]\nTP :  0 TN :  1618\nclass 1 :  [[1708   11]\n [  67    7]]\nTP :  7 TN :  1708\nclass 2 :  [[1500  145]\n [ 118   30]]\nTP :  30 TN :  1500\nclass 3 :  [[1351  270]\n [ 113   59]]\nTP :  59 TN :  1351\nclass 4 :  [[1334  340]\n [  74   45]]\nTP :  45 TN :  1334\nclass 5 :  [[1134  254]\n [ 259  146]]\nTP :  146 TN :  1134\nclass 6 :  [[1446  202]\n [  95   50]]\nTP :  50 TN :  1446\nclass 7 :  [[1508  119]\n [ 123   43]]\nTP :  43 TN :  1508\nclass 8 :  [[1595   57]\n [ 134    7]]\nTP :  7 TN :  1595\nclass 9 :  [[1700    3]\n [  86    4]]\nTP :  4 TN :  1700\nclass 10 :  [[1634    0]\n [ 159    0]]\nTP :  0 TN :  1634\n AVG accuracy : 0.2180702732849972\nperformance of the model on the test basis\n--------------------------------------\nmean square error 2.3579881409302206\nR2 score 0.34264851907679816\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00       174\n           1       0.39      0.09      0.15        74\n           2       0.17      0.20      0.19       148\n           3       0.18      0.34      0.24       172\n           4       0.12      0.38      0.18       119\n           5       0.36      0.36      0.36       405\n           6       0.20      0.34      0.25       145\n           7       0.27      0.26      0.26       166\n           8       0.11      0.05      0.07       141\n           9       0.57      0.04      0.08        90\n          10       0.00      0.00      0.00       159\n\n    accuracy                           0.22      1793\n   macro avg       0.22      0.19      0.16      1793\nweighted avg       0.22      0.22      0.19      1793\n\n/root/work/library.py:413: RuntimeWarning: invalid value encountered in long_scalars\n  pre = (TP) / (TP + FP)\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n0 :  {'TN': 1604, 'FN': 170, 'FP': 15, 'TP': 4}\n1 :  {'TN': 1558, 'FN': 41, 'FP': 161, 'TP': 33}\n2 :  {'TN': 1211, 'FN': 60, 'FP': 434, 'TP': 88}\n3 :  {'TN': 876, 'FN': 28, 'FP': 745, 'TP': 144}\n4 :  {'TN': 668, 'FN': 11, 'FP': 1006, 'TP': 108}\n5 :  {'TN': 637, 'FN': 119, 'FP': 751, 'TP': 286}\n6 :  {'TN': 924, 'FN': 55, 'FP': 724, 'TP': 90}\n7 :  {'TN': 1247, 'FN': 68, 'FP': 380, 'TP': 98}\n8 :  {'TN': 1473, 'FN': 87, 'FP': 179, 'TP': 54}\n9 :  {'TN': 1652, 'FN': 70, 'FP': 51, 'TP': 20}\n10 :  {'TN': 1628, 'FN': 158, 'FP': 6, 'TP': 1}\nclass | precision | recall | f1-score | accuracy\n0     |  0.211     |  0.023  |  0.041    |  0.897\n1     |  0.17     |  0.446  |  0.246    |  0.887\n2     |  0.169     |  0.595  |  0.263    |  0.724\n3     |  0.162     |  0.837  |  0.271    |  0.569\n4     |  0.097     |  0.908  |  0.175    |  0.433\n5     |  0.276     |  0.706  |  0.397    |  0.515\n6     |  0.111     |  0.621  |  0.188    |  0.566\n7     |  0.205     |  0.59  |  0.304    |  0.75\n8     |  0.232     |  0.383  |  0.289    |  0.852\n9     |  0.282     |  0.222  |  0.248    |  0.933\n10     |  0.143     |  0.006  |  0.012    |  0.909\n\naccuracy :     0.516\n 30%|███       | 3/10 [00:23<00:55,  7.89s/it]0 :  {'TN': 1604, 'FN': 170, 'FP': 15, 'TP': 4}\n1 :  {'TN': 1558, 'FN': 41, 'FP': 161, 'TP': 33}\n2 :  {'TN': 1211, 'FN': 60, 'FP': 434, 'TP': 88}\n3 :  {'TN': 876, 'FN': 28, 'FP': 745, 'TP': 144}\n4 :  {'TN': 668, 'FN': 11, 'FP': 1006, 'TP': 108}\n5 :  {'TN': 637, 'FN': 119, 'FP': 751, 'TP': 286}\n6 :  {'TN': 924, 'FN': 55, 'FP': 724, 'TP': 90}\n7 :  {'TN': 1247, 'FN': 68, 'FP': 380, 'TP': 98}\n8 :  {'TN': 1473, 'FN': 87, 'FP': 179, 'TP': 54}\n9 :  {'TN': 1652, 'FN': 70, 'FP': 51, 'TP': 20}\n10 :  {'TN': 1628, 'FN': 158, 'FP': 6, 'TP': 1}\nclass | precision | recall | f1-score | accuracy\n0     |  0.211     |  0.023  |  0.041    |  0.897\n1     |  0.17     |  0.446  |  0.246    |  0.887\n2     |  0.169     |  0.595  |  0.263    |  0.724\n3     |  0.162     |  0.837  |  0.271    |  0.569\n4     |  0.097     |  0.908  |  0.175    |  0.433\n5     |  0.276     |  0.706  |  0.397    |  0.515\n6     |  0.111     |  0.621  |  0.188    |  0.566\n7     |  0.205     |  0.59  |  0.304    |  0.75\n8     |  0.232     |  0.383  |  0.289    |  0.852\n9     |  0.282     |  0.222  |  0.248    |  0.933\n10     |  0.143     |  0.006  |  0.012    |  0.909\n\naccuracy :     0.516\n\n\n Modèle n°3 : FINISHED \n\n Modèle n°4 : STARTED \n\nclass 0 :  [[1624    4]\n [ 164    1]]\nTP :  1 TN :  1624\nclass 1 :  [[1680   24]\n [  85    4]]\nTP :  4 TN :  1680\nclass 2 :  [[1498  159]\n [ 109   27]]\nTP :  27 TN :  1498\nclass 3 :  [[1342  293]\n [ 109   49]]\nTP :  49 TN :  1342\nclass 4 :  [[1338  327]\n [  90   38]]\nTP :  38 TN :  1338\nclass 5 :  [[1149  196]\n [ 268  180]]\nTP :  180 TN :  1149\nclass 6 :  [[1463  202]\n [  85   43]]\nTP :  43 TN :  1463\nclass 7 :  [[1492  121]\n [ 141   39]]\nTP :  39 TN :  1492\nclass 8 :  [[1603   54]\n [ 114   22]]\nTP :  22 TN :  1603\nclass 9 :  [[1704    6]\n [  80    3]]\nTP :  3 TN :  1704\nclass 10 :  [[1651    0]\n [ 141    1]]\nTP :  1 TN :  1651\n AVG accuracy : 0.22699386503067484\nperformance of the model on the test basis\n--------------------------------------\nmean square error 2.2975301187484534\nR2 score 0.34627491869150195\n              precision    recall  f1-score   support\n\n           0       0.20      0.01      0.01       165\n           1       0.14      0.04      0.07        89\n           2       0.15      0.20      0.17       136\n           3       0.14      0.31      0.20       158\n           4       0.10      0.30      0.15       128\n           5       0.48      0.40      0.44       448\n           6       0.18      0.34      0.23       128\n           7       0.24      0.22      0.23       180\n           8       0.29      0.16      0.21       136\n           9       0.33      0.04      0.07        83\n          10       1.00      0.01      0.01       142\n\n    accuracy                           0.23      1793\n   macro avg       0.30      0.18      0.16      1793\nweighted avg       0.33      0.23      0.21      1793\n\n0 :  {'TN': 1602, 'FN': 158, 'FP': 26, 'TP': 7}\n1 :  {'TN': 1520, 'FN': 54, 'FP': 184, 'TP': 35}\n2 :  {'TN': 1190, 'FN': 47, 'FP': 467, 'TP': 89}\n3 :  {'TN': 863, 'FN': 37, 'FP': 772, 'TP': 121}\n4 :  {'TN': 689, 'FN': 21, 'FP': 976, 'TP': 107}\n5 :  {'TN': 682, 'FN': 125, 'FP': 663, 'TP': 323}\n6 :  {'TN': 960, 'FN': 52, 'FP': 705, 'TP': 76}\n7 :  {'TN': 1231, 'FN': 81, 'FP': 382, 'TP': 99}\n8 :  {'TN': 1473, 'FN': 75, 'FP': 184, 'TP': 61}\n9 :  {'TN': 1640, 'FN': 67, 'FP': 70, 'TP': 16}\n10 :  {'TN': 1647, 'FN': 136, 'FP': 4, 'TP': 6}\nclass | precision | recall | f1-score | accuracy\n0     |  0.212     |  0.042  |  0.07    |  0.897\n1     |  0.16     |  0.393  |  0.227    |  0.867\n2     |  0.16     |  0.654  |  0.257    |  0.713\n3     |  0.135     |  0.766  |  0.23    |  0.549\n4     |  0.099     |  0.836  |  0.177    |  0.444\n5     |  0.328     |  0.721  |  0.451    |  0.561\n6     |  0.097     |  0.594  |  0.167    |  0.578\n7     |  0.206     |  0.55  |  0.3    |  0.742\n8     |  0.249     |  0.449  |  0.32    |  0.856\n9     |  0.186     |  0.193  |  0.189    |  0.924\n10     |  0.6     |  0.042  |  0.079    |  0.922\n\naccuracy :     0.524\n 40%|████      | 4/10 [00:31<00:47,  7.85s/it]0 :  {'TN': 1602, 'FN': 158, 'FP': 26, 'TP': 7}\n1 :  {'TN': 1520, 'FN': 54, 'FP': 184, 'TP': 35}\n2 :  {'TN': 1190, 'FN': 47, 'FP': 467, 'TP': 89}\n3 :  {'TN': 863, 'FN': 37, 'FP': 772, 'TP': 121}\n4 :  {'TN': 689, 'FN': 21, 'FP': 976, 'TP': 107}\n5 :  {'TN': 682, 'FN': 125, 'FP': 663, 'TP': 323}\n6 :  {'TN': 960, 'FN': 52, 'FP': 705, 'TP': 76}\n7 :  {'TN': 1231, 'FN': 81, 'FP': 382, 'TP': 99}\n8 :  {'TN': 1473, 'FN': 75, 'FP': 184, 'TP': 61}\n9 :  {'TN': 1640, 'FN': 67, 'FP': 70, 'TP': 16}\n10 :  {'TN': 1647, 'FN': 136, 'FP': 4, 'TP': 6}\nclass | precision | recall | f1-score | accuracy\n0     |  0.212     |  0.042  |  0.07    |  0.897\n1     |  0.16     |  0.393  |  0.227    |  0.867\n2     |  0.16     |  0.654  |  0.257    |  0.713\n3     |  0.135     |  0.766  |  0.23    |  0.549\n4     |  0.099     |  0.836  |  0.177    |  0.444\n5     |  0.328     |  0.721  |  0.451    |  0.561\n6     |  0.097     |  0.594  |  0.167    |  0.578\n7     |  0.206     |  0.55  |  0.3    |  0.742\n8     |  0.249     |  0.449  |  0.32    |  0.856\n9     |  0.186     |  0.193  |  0.189    |  0.924\n10     |  0.6     |  0.042  |  0.079    |  0.922\n\naccuracy :     0.524\n\n\n Modèle n°4 : FINISHED \n\n Modèle n°5 : STARTED \n\nclass 0 :  [[1614    2]\n [ 176    1]]\nTP :  1 TN :  1614\nclass 1 :  [[1682   27]\n [  80    4]]\nTP :  4 TN :  1682\nclass 2 :  [[1511  142]\n [ 107   33]]\nTP :  33 TN :  1511\nclass 3 :  [[1324  295]\n [ 113   61]]\nTP :  61 TN :  1324\nclass 4 :  [[1347  320]\n [  75   51]]\nTP :  51 TN :  1347\nclass 5 :  [[1134  215]\n [ 276  168]]\nTP :  168 TN :  1134\nclass 6 :  [[1483  187]\n [  81   42]]\nTP :  42 TN :  1483\nclass 7 :  [[1539  112]\n [ 104   38]]\nTP :  38 TN :  1539\nclass 8 :  [[1602   60]\n [ 112   19]]\nTP :  19 TN :  1602\nclass 9 :  [[1692   10]\n [  85    6]]\nTP :  6 TN :  1692\nclass 10 :  [[1632    0]\n [ 161    0]]\nTP :  0 TN :  1632\n AVG accuracy : 0.23591745677635248\nperformance of the model on the test basis\n--------------------------------------\nmean square error 2.3282508191702203\nR2 score 0.3601489964919481\n              precision    recall  f1-score   support\n\n           0       0.33      0.01      0.01       177\n           1       0.13      0.05      0.07        84\n           2       0.19      0.24      0.21       140\n           3       0.17      0.35      0.23       174\n           4       0.14      0.40      0.21       126\n           5       0.44      0.38      0.41       444\n           6       0.18      0.34      0.24       123\n           7       0.25      0.27      0.26       142\n           8       0.24      0.15      0.18       131\n           9       0.38      0.07      0.11        91\n          10       0.00      0.00      0.00       161\n\n    accuracy                           0.24      1793\n   macro avg       0.22      0.20      0.17      1793\nweighted avg       0.26      0.24      0.21      1793\n\n/root/work/library.py:413: RuntimeWarning: invalid value encountered in long_scalars\n  pre = (TP) / (TP + FP)\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n0 :  {'TN': 1589, 'FN': 170, 'FP': 27, 'TP': 7}\n1 :  {'TN': 1537, 'FN': 47, 'FP': 172, 'TP': 37}\n2 :  {'TN': 1192, 'FN': 39, 'FP': 461, 'TP': 101}\n3 :  {'TN': 850, 'FN': 41, 'FP': 769, 'TP': 133}\n4 :  {'TN': 669, 'FN': 14, 'FP': 998, 'TP': 112}\n5 :  {'TN': 694, 'FN': 116, 'FP': 655, 'TP': 328}\n6 :  {'TN': 987, 'FN': 44, 'FP': 683, 'TP': 79}\n7 :  {'TN': 1278, 'FN': 57, 'FP': 373, 'TP': 85}\n8 :  {'TN': 1473, 'FN': 75, 'FP': 189, 'TP': 56}\n9 :  {'TN': 1632, 'FN': 66, 'FP': 70, 'TP': 25}\n10 :  {'TN': 1622, 'FN': 155, 'FP': 10, 'TP': 6}\nclass | precision | recall | f1-score | accuracy\n0     |  0.206     |  0.04  |  0.067    |  0.89\n1     |  0.177     |  0.44  |  0.252    |  0.878\n2     |  0.18     |  0.721  |  0.288    |  0.721\n3     |  0.147     |  0.764  |  0.247    |  0.548\n4     |  0.101     |  0.889  |  0.181    |  0.436\n5     |  0.334     |  0.739  |  0.46    |  0.57\n6     |  0.104     |  0.642  |  0.179    |  0.595\n7     |  0.186     |  0.599  |  0.284    |  0.76\n8     |  0.229     |  0.427  |  0.298    |  0.853\n9     |  0.263     |  0.275  |  0.269    |  0.924\n10     |  0.375     |  0.037  |  0.067    |  0.908\n\naccuracy :     0.54\n 50%|█████     | 5/10 [00:39<00:39,  7.92s/it]0 :  {'TN': 1589, 'FN': 170, 'FP': 27, 'TP': 7}\n1 :  {'TN': 1537, 'FN': 47, 'FP': 172, 'TP': 37}\n2 :  {'TN': 1192, 'FN': 39, 'FP': 461, 'TP': 101}\n3 :  {'TN': 850, 'FN': 41, 'FP': 769, 'TP': 133}\n4 :  {'TN': 669, 'FN': 14, 'FP': 998, 'TP': 112}\n5 :  {'TN': 694, 'FN': 116, 'FP': 655, 'TP': 328}\n6 :  {'TN': 987, 'FN': 44, 'FP': 683, 'TP': 79}\n7 :  {'TN': 1278, 'FN': 57, 'FP': 373, 'TP': 85}\n8 :  {'TN': 1473, 'FN': 75, 'FP': 189, 'TP': 56}\n9 :  {'TN': 1632, 'FN': 66, 'FP': 70, 'TP': 25}\n10 :  {'TN': 1622, 'FN': 155, 'FP': 10, 'TP': 6}\nclass | precision | recall | f1-score | accuracy\n0     |  0.206     |  0.04  |  0.067    |  0.89\n1     |  0.177     |  0.44  |  0.252    |  0.878\n2     |  0.18     |  0.721  |  0.288    |  0.721\n3     |  0.147     |  0.764  |  0.247    |  0.548\n4     |  0.101     |  0.889  |  0.181    |  0.436\n5     |  0.334     |  0.739  |  0.46    |  0.57\n6     |  0.104     |  0.642  |  0.179    |  0.595\n7     |  0.186     |  0.599  |  0.284    |  0.76\n8     |  0.229     |  0.427  |  0.298    |  0.853\n9     |  0.263     |  0.275  |  0.269    |  0.924\n10     |  0.375     |  0.037  |  0.067    |  0.908\n\naccuracy :     0.54\n\n\n Modèle n°5 : FINISHED \n\n Modèle n°6 : STARTED \n\nclass 0 :  [[1619    2]\n [ 172    0]]\nTP :  0 TN :  1619\nclass 1 :  [[1677   20]\n [  90    6]]\nTP :  6 TN :  1677\nclass 2 :  [[1495  172]\n [ 101   25]]\nTP :  25 TN :  1495\nclass 3 :  [[1331  283]\n [ 118   61]]\nTP :  61 TN :  1331\nclass 4 :  [[1341  328]\n [  85   39]]\nTP :  39 TN :  1341\nclass 5 :  [[1138  218]\n [ 266  171]]\nTP :  171 TN :  1138\nclass 6 :  [[1477  189]\n [  82   45]]\nTP :  45 TN :  1477\nclass 7 :  [[1550   91]\n [ 112   40]]\nTP :  40 TN :  1550\nclass 8 :  [[1587   78]\n [ 116   12]]\nTP :  12 TN :  1587\nclass 9 :  [[1687    7]\n [  93    6]]\nTP :  6 TN :  1687\nclass 10 :  [[1640    0]\n [ 153    0]]\nTP :  0 TN :  1640\n AVG accuracy : 0.22587841606246514\nperformance of the model on the test basis\n--------------------------------------\nmean square error 2.38672733492955\nR2 score 0.3234510699653722\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00       172\n           1       0.23      0.06      0.10        96\n           2       0.13      0.20      0.15       126\n           3       0.18      0.34      0.23       179\n           4       0.11      0.31      0.16       124\n           5       0.44      0.39      0.41       437\n           6       0.19      0.35      0.25       127\n           7       0.31      0.26      0.28       152\n           8       0.13      0.09      0.11       128\n           9       0.46      0.06      0.11        99\n          10       0.00      0.00      0.00       153\n\n    accuracy                           0.23      1793\n   macro avg       0.20      0.19      0.16      1793\nweighted avg       0.23      0.23      0.21      1793\n\n/root/work/library.py:413: RuntimeWarning: invalid value encountered in long_scalars\n  pre = (TP) / (TP + FP)\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n0 :  {'TN': 1603, 'FN': 162, 'FP': 18, 'TP': 10}\n1 :  {'TN': 1516, 'FN': 52, 'FP': 181, 'TP': 44}\n2 :  {'TN': 1183, 'FN': 43, 'FP': 484, 'TP': 83}\n3 :  {'TN': 860, 'FN': 25, 'FP': 754, 'TP': 154}\n4 :  {'TN': 671, 'FN': 22, 'FP': 998, 'TP': 102}\n5 :  {'TN': 692, 'FN': 111, 'FP': 664, 'TP': 326}\n6 :  {'TN': 993, 'FN': 46, 'FP': 673, 'TP': 81}\n7 :  {'TN': 1263, 'FN': 75, 'FP': 378, 'TP': 77}\n8 :  {'TN': 1474, 'FN': 85, 'FP': 191, 'TP': 43}\n9 :  {'TN': 1624, 'FN': 66, 'FP': 70, 'TP': 33}\n10 :  {'TN': 1634, 'FN': 146, 'FP': 6, 'TP': 7}\nclass | precision | recall | f1-score | accuracy\n0     |  0.357     |  0.058  |  0.1    |  0.9\n1     |  0.196     |  0.458  |  0.275    |  0.87\n2     |  0.146     |  0.659  |  0.239    |  0.706\n3     |  0.17     |  0.86  |  0.284    |  0.566\n4     |  0.093     |  0.823  |  0.167    |  0.431\n5     |  0.329     |  0.746  |  0.457    |  0.568\n6     |  0.107     |  0.638  |  0.183    |  0.599\n7     |  0.169     |  0.507  |  0.254    |  0.747\n8     |  0.184     |  0.336  |  0.238    |  0.846\n9     |  0.32     |  0.333  |  0.326    |  0.924\n10     |  0.538     |  0.046  |  0.085    |  0.915\n\naccuracy :     0.535\n 60%|██████    | 6/10 [00:47<00:31,  7.94s/it]0 :  {'TN': 1603, 'FN': 162, 'FP': 18, 'TP': 10}\n1 :  {'TN': 1516, 'FN': 52, 'FP': 181, 'TP': 44}\n2 :  {'TN': 1183, 'FN': 43, 'FP': 484, 'TP': 83}\n3 :  {'TN': 860, 'FN': 25, 'FP': 754, 'TP': 154}\n4 :  {'TN': 671, 'FN': 22, 'FP': 998, 'TP': 102}\n5 :  {'TN': 692, 'FN': 111, 'FP': 664, 'TP': 326}\n6 :  {'TN': 993, 'FN': 46, 'FP': 673, 'TP': 81}\n7 :  {'TN': 1263, 'FN': 75, 'FP': 378, 'TP': 77}\n8 :  {'TN': 1474, 'FN': 85, 'FP': 191, 'TP': 43}\n9 :  {'TN': 1624, 'FN': 66, 'FP': 70, 'TP': 33}\n10 :  {'TN': 1634, 'FN': 146, 'FP': 6, 'TP': 7}\nclass | precision | recall | f1-score | accuracy\n0     |  0.357     |  0.058  |  0.1    |  0.9\n1     |  0.196     |  0.458  |  0.275    |  0.87\n2     |  0.146     |  0.659  |  0.239    |  0.706\n3     |  0.17     |  0.86  |  0.284    |  0.566\n4     |  0.093     |  0.823  |  0.167    |  0.431\n5     |  0.329     |  0.746  |  0.457    |  0.568\n6     |  0.107     |  0.638  |  0.183    |  0.599\n7     |  0.169     |  0.507  |  0.254    |  0.747\n8     |  0.184     |  0.336  |  0.238    |  0.846\n9     |  0.32     |  0.333  |  0.326    |  0.924\n10     |  0.538     |  0.046  |  0.085    |  0.915\n\naccuracy :     0.535\n\n\n Modèle n°6 : FINISHED \n\n Modèle n°7 : STARTED \n\nclass 0 :  [[1633    2]\n [ 158    0]]\nTP :  0 TN :  1633\nclass 1 :  [[1697   16]\n [  74    6]]\nTP :  6 TN :  1697\nclass 2 :  [[1526  141]\n [  98   28]]\nTP :  28 TN :  1526\nclass 3 :  [[1331  294]\n [ 111   57]]\nTP :  57 TN :  1331\nclass 4 :  [[1304  355]\n [  88   46]]\nTP :  46 TN :  1304\nclass 5 :  [[1137  203]\n [ 282  171]]\nTP :  171 TN :  1137\nclass 6 :  [[1489  175]\n [  78   51]]\nTP :  51 TN :  1489\nclass 7 :  [[1509  125]\n [ 124   35]]\nTP :  35 TN :  1509\nclass 8 :  [[1577   63]\n [ 138   15]]\nTP :  15 TN :  1577\nclass 9 :  [[1708    6]\n [  75    4]]\nTP :  4 TN :  1708\nclass 10 :  [[1639    0]\n [ 154    0]]\nTP :  0 TN :  1639\n AVG accuracy : 0.23034021193530396\nperformance of the model on the test basis\n--------------------------------------\nmean square error 2.3513328948421566\nR2 score 0.31280180365065224\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00       158\n           1       0.27      0.07      0.12        80\n           2       0.17      0.22      0.19       126\n           3       0.16      0.34      0.22       168\n           4       0.11      0.34      0.17       134\n           5       0.46      0.38      0.41       453\n           6       0.23      0.40      0.29       129\n           7       0.22      0.22      0.22       159\n           8       0.19      0.10      0.13       153\n           9       0.40      0.05      0.09        79\n          10       0.00      0.00      0.00       154\n\n    accuracy                           0.23      1793\n   macro avg       0.20      0.19      0.17      1793\nweighted avg       0.23      0.23      0.21      1793\n\n/root/work/library.py:413: RuntimeWarning: invalid value encountered in long_scalars\n  pre = (TP) / (TP + FP)\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n0 :  {'TN': 1615, 'FN': 154, 'FP': 20, 'TP': 4}\n1 :  {'TN': 1553, 'FN': 47, 'FP': 160, 'TP': 33}\n2 :  {'TN': 1206, 'FN': 45, 'FP': 461, 'TP': 81}\n3 :  {'TN': 854, 'FN': 18, 'FP': 771, 'TP': 150}\n4 :  {'TN': 639, 'FN': 28, 'FP': 1020, 'TP': 106}\n5 :  {'TN': 669, 'FN': 123, 'FP': 671, 'TP': 330}\n6 :  {'TN': 988, 'FN': 45, 'FP': 676, 'TP': 84}\n7 :  {'TN': 1253, 'FN': 76, 'FP': 381, 'TP': 83}\n8 :  {'TN': 1455, 'FN': 90, 'FP': 185, 'TP': 63}\n9 :  {'TN': 1650, 'FN': 55, 'FP': 64, 'TP': 24}\n10 :  {'TN': 1633, 'FN': 150, 'FP': 6, 'TP': 4}\nclass | precision | recall | f1-score | accuracy\n0     |  0.167     |  0.025  |  0.043    |  0.903\n1     |  0.171     |  0.412  |  0.242    |  0.885\n2     |  0.149     |  0.643  |  0.242    |  0.718\n3     |  0.163     |  0.893  |  0.276    |  0.56\n4     |  0.094     |  0.791  |  0.168    |  0.416\n5     |  0.33     |  0.728  |  0.454    |  0.557\n6     |  0.111     |  0.651  |  0.19    |  0.598\n7     |  0.179     |  0.522  |  0.267    |  0.745\n8     |  0.254     |  0.412  |  0.314    |  0.847\n9     |  0.273     |  0.304  |  0.288    |  0.934\n10     |  0.4     |  0.026  |  0.049    |  0.913\n\naccuracy :     0.537\n 70%|███████   | 7/10 [00:55<00:23,  7.82s/it]0 :  {'TN': 1615, 'FN': 154, 'FP': 20, 'TP': 4}\n1 :  {'TN': 1553, 'FN': 47, 'FP': 160, 'TP': 33}\n2 :  {'TN': 1206, 'FN': 45, 'FP': 461, 'TP': 81}\n3 :  {'TN': 854, 'FN': 18, 'FP': 771, 'TP': 150}\n4 :  {'TN': 639, 'FN': 28, 'FP': 1020, 'TP': 106}\n5 :  {'TN': 669, 'FN': 123, 'FP': 671, 'TP': 330}\n6 :  {'TN': 988, 'FN': 45, 'FP': 676, 'TP': 84}\n7 :  {'TN': 1253, 'FN': 76, 'FP': 381, 'TP': 83}\n8 :  {'TN': 1455, 'FN': 90, 'FP': 185, 'TP': 63}\n9 :  {'TN': 1650, 'FN': 55, 'FP': 64, 'TP': 24}\n10 :  {'TN': 1633, 'FN': 150, 'FP': 6, 'TP': 4}\nclass | precision | recall | f1-score | accuracy\n0     |  0.167     |  0.025  |  0.043    |  0.903\n1     |  0.171     |  0.412  |  0.242    |  0.885\n2     |  0.149     |  0.643  |  0.242    |  0.718\n3     |  0.163     |  0.893  |  0.276    |  0.56\n4     |  0.094     |  0.791  |  0.168    |  0.416\n5     |  0.33     |  0.728  |  0.454    |  0.557\n6     |  0.111     |  0.651  |  0.19    |  0.598\n7     |  0.179     |  0.522  |  0.267    |  0.745\n8     |  0.254     |  0.412  |  0.314    |  0.847\n9     |  0.273     |  0.304  |  0.288    |  0.934\n10     |  0.4     |  0.026  |  0.049    |  0.913\n\naccuracy :     0.537\n\n\n Modèle n°7 : FINISHED \n\n Modèle n°8 : STARTED \n\nclass 0 :  [[1610    3]\n [ 180    0]]\nTP :  0 TN :  1610\nclass 1 :  [[1697   14]\n [  78    4]]\nTP :  4 TN :  1697\nclass 2 :  [[1496  165]\n [ 108   24]]\nTP :  24 TN :  1496\nclass 3 :  [[1322  308]\n [ 100   63]]\nTP :  63 TN :  1322\nclass 4 :  [[1326  324]\n [  99   44]]\nTP :  44 TN :  1326\nclass 5 :  [[1178  197]\n [ 254  164]]\nTP :  164 TN :  1178\nclass 6 :  [[1456  184]\n [  98   55]]\nTP :  55 TN :  1456\nclass 7 :  [[1535  107]\n [ 117   34]]\nTP :  34 TN :  1535\nclass 8 :  [[1586   76]\n [ 117   14]]\nTP :  14 TN :  1586\nclass 9 :  [[1712    8]\n [  68    5]]\nTP :  5 TN :  1712\nclass 10 :  [[1626    0]\n [ 167    0]]\nTP :  0 TN :  1626\n AVG accuracy : 0.22699386503067484\nperformance of the model on the test basis\n--------------------------------------\nmean square error 2.3505260980955005\nR2 score 0.34248490253730735\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00       180\n           1       0.22      0.05      0.08        82\n           2       0.13      0.18      0.15       132\n           3       0.17      0.39      0.24       163\n           4       0.12      0.31      0.17       143\n           5       0.45      0.39      0.42       418\n           6       0.23      0.36      0.28       153\n           7       0.24      0.23      0.23       151\n           8       0.16      0.11      0.13       131\n           9       0.38      0.07      0.12        73\n          10       0.00      0.00      0.00       167\n\n    accuracy                           0.23      1793\n   macro avg       0.19      0.19      0.17      1793\nweighted avg       0.22      0.23      0.21      1793\n\n/root/work/library.py:413: RuntimeWarning: invalid value encountered in long_scalars\n  pre = (TP) / (TP + FP)\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n0 :  {'TN': 1600, 'FN': 172, 'FP': 13, 'TP': 8}\n1 :  {'TN': 1528, 'FN': 55, 'FP': 183, 'TP': 27}\n2 :  {'TN': 1171, 'FN': 44, 'FP': 490, 'TP': 88}\n3 :  {'TN': 840, 'FN': 25, 'FP': 790, 'TP': 138}\n4 :  {'TN': 673, 'FN': 20, 'FP': 977, 'TP': 123}\n5 :  {'TN': 712, 'FN': 113, 'FP': 663, 'TP': 305}\n6 :  {'TN': 990, 'FN': 62, 'FP': 650, 'TP': 91}\n7 :  {'TN': 1268, 'FN': 55, 'FP': 374, 'TP': 96}\n8 :  {'TN': 1469, 'FN': 80, 'FP': 193, 'TP': 51}\n9 :  {'TN': 1638, 'FN': 52, 'FP': 82, 'TP': 21}\n10 :  {'TN': 1618, 'FN': 162, 'FP': 8, 'TP': 5}\nclass | precision | recall | f1-score | accuracy\n0     |  0.381     |  0.044  |  0.079    |  0.897\n1     |  0.129     |  0.329  |  0.185    |  0.867\n2     |  0.152     |  0.667  |  0.248    |  0.702\n3     |  0.149     |  0.847  |  0.253    |  0.545\n4     |  0.112     |  0.86  |  0.198    |  0.444\n5     |  0.315     |  0.73  |  0.44    |  0.567\n6     |  0.123     |  0.595  |  0.204    |  0.603\n7     |  0.204     |  0.636  |  0.309    |  0.761\n8     |  0.209     |  0.389  |  0.272    |  0.848\n9     |  0.204     |  0.288  |  0.239    |  0.925\n10     |  0.385     |  0.03  |  0.056    |  0.905\n\naccuracy :     0.532\n 80%|████████  | 8/10 [01:02<00:15,  7.75s/it]0 :  {'TN': 1600, 'FN': 172, 'FP': 13, 'TP': 8}\n1 :  {'TN': 1528, 'FN': 55, 'FP': 183, 'TP': 27}\n2 :  {'TN': 1171, 'FN': 44, 'FP': 490, 'TP': 88}\n3 :  {'TN': 840, 'FN': 25, 'FP': 790, 'TP': 138}\n4 :  {'TN': 673, 'FN': 20, 'FP': 977, 'TP': 123}\n5 :  {'TN': 712, 'FN': 113, 'FP': 663, 'TP': 305}\n6 :  {'TN': 990, 'FN': 62, 'FP': 650, 'TP': 91}\n7 :  {'TN': 1268, 'FN': 55, 'FP': 374, 'TP': 96}\n8 :  {'TN': 1469, 'FN': 80, 'FP': 193, 'TP': 51}\n9 :  {'TN': 1638, 'FN': 52, 'FP': 82, 'TP': 21}\n10 :  {'TN': 1618, 'FN': 162, 'FP': 8, 'TP': 5}\nclass | precision | recall | f1-score | accuracy\n0     |  0.381     |  0.044  |  0.079    |  0.897\n1     |  0.129     |  0.329  |  0.185    |  0.867\n2     |  0.152     |  0.667  |  0.248    |  0.702\n3     |  0.149     |  0.847  |  0.253    |  0.545\n4     |  0.112     |  0.86  |  0.198    |  0.444\n5     |  0.315     |  0.73  |  0.44    |  0.567\n6     |  0.123     |  0.595  |  0.204    |  0.603\n7     |  0.204     |  0.636  |  0.309    |  0.761\n8     |  0.209     |  0.389  |  0.272    |  0.848\n9     |  0.204     |  0.288  |  0.239    |  0.925\n10     |  0.385     |  0.03  |  0.056    |  0.905\n\naccuracy :     0.532\n\n\n Modèle n°8 : FINISHED \n\n Modèle n°9 : STARTED \n\nclass 0 :  [[1640    2]\n [ 151    0]]\nTP :  0 TN :  1640\nclass 1 :  [[1681   17]\n [  90    5]]\nTP :  5 TN :  1681\nclass 2 :  [[1511  152]\n [  99   31]]\nTP :  31 TN :  1511\nclass 3 :  [[1365  274]\n [ 103   51]]\nTP :  51 TN :  1365\nclass 4 :  [[1277  376]\n [  98   42]]\nTP :  42 TN :  1277\nclass 5 :  [[1145  204]\n [ 272  172]]\nTP :  172 TN :  1145\nclass 6 :  [[1489  162]\n [  97   45]]\nTP :  45 TN :  1489\nclass 7 :  [[1500  132]\n [ 126   35]]\nTP :  35 TN :  1500\nclass 8 :  [[1593   66]\n [ 118   16]]\nTP :  16 TN :  1593\nclass 9 :  [[1692    5]\n [  91    5]]\nTP :  5 TN :  1692\nclass 10 :  [[1647    0]\n [ 145    1]]\nTP :  1 TN :  1647\n AVG accuracy : 0.22476296709425544\nperformance of the model on the test basis\n--------------------------------------\nmean square error 2.280057725661142\nR2 score 0.3526480403669612\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00       151\n           1       0.23      0.05      0.09        95\n           2       0.17      0.24      0.20       130\n           3       0.16      0.33      0.21       154\n           4       0.10      0.30      0.15       140\n           5       0.46      0.39      0.42       444\n           6       0.22      0.32      0.26       142\n           7       0.21      0.22      0.21       161\n           8       0.20      0.12      0.15       134\n           9       0.50      0.05      0.09        96\n          10       1.00      0.01      0.01       146\n\n    accuracy                           0.22      1793\n   macro avg       0.29      0.18      0.16      1793\nweighted avg       0.32      0.22      0.21      1793\n\n0 :  {'TN': 1625, 'FN': 144, 'FP': 17, 'TP': 7}\n1 :  {'TN': 1532, 'FN': 54, 'FP': 166, 'TP': 41}\n2 :  {'TN': 1215, 'FN': 48, 'FP': 448, 'TP': 82}\n3 :  {'TN': 845, 'FN': 22, 'FP': 794, 'TP': 132}\n4 :  {'TN': 658, 'FN': 16, 'FP': 995, 'TP': 124}\n5 :  {'TN': 675, 'FN': 117, 'FP': 674, 'TP': 327}\n6 :  {'TN': 988, 'FN': 55, 'FP': 663, 'TP': 87}\n7 :  {'TN': 1263, 'FN': 74, 'FP': 369, 'TP': 87}\n8 :  {'TN': 1462, 'FN': 72, 'FP': 197, 'TP': 62}\n9 :  {'TN': 1634, 'FN': 66, 'FP': 63, 'TP': 30}\n10 :  {'TN': 1639, 'FN': 143, 'FP': 8, 'TP': 3}\nclass | precision | recall | f1-score | accuracy\n0     |  0.292     |  0.046  |  0.079    |  0.91\n1     |  0.198     |  0.432  |  0.272    |  0.877\n2     |  0.155     |  0.631  |  0.249    |  0.723\n3     |  0.143     |  0.857  |  0.245    |  0.545\n4     |  0.111     |  0.886  |  0.197    |  0.436\n5     |  0.327     |  0.736  |  0.453    |  0.559\n6     |  0.116     |  0.613  |  0.195    |  0.6\n7     |  0.191     |  0.54  |  0.282    |  0.753\n8     |  0.239     |  0.463  |  0.315    |  0.85\n9     |  0.323     |  0.312  |  0.317    |  0.928\n10     |  0.273     |  0.021  |  0.039    |  0.916\n\naccuracy :     0.548\n 90%|█████████ | 9/10 [01:10<00:07,  7.69s/it]0 :  {'TN': 1625, 'FN': 144, 'FP': 17, 'TP': 7}\n1 :  {'TN': 1532, 'FN': 54, 'FP': 166, 'TP': 41}\n2 :  {'TN': 1215, 'FN': 48, 'FP': 448, 'TP': 82}\n3 :  {'TN': 845, 'FN': 22, 'FP': 794, 'TP': 132}\n4 :  {'TN': 658, 'FN': 16, 'FP': 995, 'TP': 124}\n5 :  {'TN': 675, 'FN': 117, 'FP': 674, 'TP': 327}\n6 :  {'TN': 988, 'FN': 55, 'FP': 663, 'TP': 87}\n7 :  {'TN': 1263, 'FN': 74, 'FP': 369, 'TP': 87}\n8 :  {'TN': 1462, 'FN': 72, 'FP': 197, 'TP': 62}\n9 :  {'TN': 1634, 'FN': 66, 'FP': 63, 'TP': 30}\n10 :  {'TN': 1639, 'FN': 143, 'FP': 8, 'TP': 3}\nclass | precision | recall | f1-score | accuracy\n0     |  0.292     |  0.046  |  0.079    |  0.91\n1     |  0.198     |  0.432  |  0.272    |  0.877\n2     |  0.155     |  0.631  |  0.249    |  0.723\n3     |  0.143     |  0.857  |  0.245    |  0.545\n4     |  0.111     |  0.886  |  0.197    |  0.436\n5     |  0.327     |  0.736  |  0.453    |  0.559\n6     |  0.116     |  0.613  |  0.195    |  0.6\n7     |  0.191     |  0.54  |  0.282    |  0.753\n8     |  0.239     |  0.463  |  0.315    |  0.85\n9     |  0.323     |  0.312  |  0.317    |  0.928\n10     |  0.273     |  0.021  |  0.039    |  0.916\n\naccuracy :     0.548\n\n\n Modèle n°9 : FINISHED \n\n Modèle n°10 : STARTED \n\nclass 0 :  [[1628    1]\n [ 163    1]]\nTP :  1 TN :  1628\nclass 1 :  [[1703   18]\n [  65    7]]\nTP :  7 TN :  1703\nclass 2 :  [[1500  151]\n [ 108   34]]\nTP :  34 TN :  1500\nclass 3 :  [[1332  313]\n [  92   56]]\nTP :  56 TN :  1332\nclass 4 :  [[1322  340]\n [  95   36]]\nTP :  36 TN :  1322\nclass 5 :  [[1158  191]\n [ 294  150]]\nTP :  150 TN :  1158\nclass 6 :  [[1462  200]\n [  88   43]]\nTP :  43 TN :  1462\nclass 7 :  [[1514  113]\n [ 122   44]]\nTP :  44 TN :  1514\nclass 8 :  [[1591   73]\n [ 116   13]]\nTP :  13 TN :  1591\nclass 9 :  [[1699    5]\n [  85    4]]\nTP :  4 TN :  1699\nclass 10 :  [[1616    0]\n [ 177    0]]\nTP :  0 TN :  1616\n AVG accuracy : 0.21639709983268265\nperformance of the model on the test basis\n--------------------------------------\nmean square error 2.3676930115537473\nR2 score 0.33206267527909017\n              precision    recall  f1-score   support\n\n           0       0.50      0.01      0.01       164\n           1       0.28      0.10      0.14        72\n           2       0.18      0.24      0.21       142\n           3       0.15      0.38      0.22       148\n           4       0.10      0.27      0.14       131\n           5       0.44      0.34      0.38       444\n           6       0.18      0.33      0.23       131\n           7       0.28      0.27      0.27       166\n           8       0.15      0.10      0.12       129\n           9       0.44      0.04      0.08        89\n          10       0.00      0.00      0.00       177\n\n    accuracy                           0.22      1793\n   macro avg       0.25      0.19      0.16      1793\nweighted avg       0.27      0.22      0.20      1793\n\n/root/work/library.py:413: RuntimeWarning: invalid value encountered in long_scalars\n  pre = (TP) / (TP + FP)\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n0 :  {'TN': 1610, 'FN': 156, 'FP': 19, 'TP': 8}\n1 :  {'TN': 1544, 'FN': 37, 'FP': 177, 'TP': 35}\n2 :  {'TN': 1163, 'FN': 51, 'FP': 488, 'TP': 91}\n3 :  {'TN': 841, 'FN': 22, 'FP': 804, 'TP': 126}\n4 :  {'TN': 685, 'FN': 22, 'FP': 977, 'TP': 109}\n5 :  {'TN': 685, 'FN': 148, 'FP': 664, 'TP': 296}\n6 :  {'TN': 996, 'FN': 56, 'FP': 666, 'TP': 75}\n7 :  {'TN': 1239, 'FN': 68, 'FP': 388, 'TP': 98}\n8 :  {'TN': 1459, 'FN': 82, 'FP': 205, 'TP': 47}\n9 :  {'TN': 1630, 'FN': 68, 'FP': 74, 'TP': 21}\n10 :  {'TN': 1610, 'FN': 174, 'FP': 6, 'TP': 3}\nclass | precision | recall | f1-score | accuracy\n0     |  0.296     |  0.049  |  0.084    |  0.902\n1     |  0.165     |  0.486  |  0.246    |  0.881\n2     |  0.157     |  0.641  |  0.252    |  0.699\n3     |  0.135     |  0.851  |  0.233    |  0.539\n4     |  0.1     |  0.832  |  0.179    |  0.443\n5     |  0.308     |  0.667  |  0.421    |  0.547\n6     |  0.101     |  0.573  |  0.172    |  0.597\n7     |  0.202     |  0.59  |  0.301    |  0.746\n8     |  0.187     |  0.364  |  0.247    |  0.84\n9     |  0.221     |  0.236  |  0.228    |  0.921\n10     |  0.333     |  0.017  |  0.032    |  0.9\n\naccuracy :     0.507\n100%|██████████| 10/10 [01:18<00:00,  7.80s/it]0 :  {'TN': 1610, 'FN': 156, 'FP': 19, 'TP': 8}\n1 :  {'TN': 1544, 'FN': 37, 'FP': 177, 'TP': 35}\n2 :  {'TN': 1163, 'FN': 51, 'FP': 488, 'TP': 91}\n3 :  {'TN': 841, 'FN': 22, 'FP': 804, 'TP': 126}\n4 :  {'TN': 685, 'FN': 22, 'FP': 977, 'TP': 109}\n5 :  {'TN': 685, 'FN': 148, 'FP': 664, 'TP': 296}\n6 :  {'TN': 996, 'FN': 56, 'FP': 666, 'TP': 75}\n7 :  {'TN': 1239, 'FN': 68, 'FP': 388, 'TP': 98}\n8 :  {'TN': 1459, 'FN': 82, 'FP': 205, 'TP': 47}\n9 :  {'TN': 1630, 'FN': 68, 'FP': 74, 'TP': 21}\n10 :  {'TN': 1610, 'FN': 174, 'FP': 6, 'TP': 3}\nclass | precision | recall | f1-score | accuracy\n0     |  0.296     |  0.049  |  0.084    |  0.902\n1     |  0.165     |  0.486  |  0.246    |  0.881\n2     |  0.157     |  0.641  |  0.252    |  0.699\n3     |  0.135     |  0.851  |  0.233    |  0.539\n4     |  0.1     |  0.832  |  0.179    |  0.443\n5     |  0.308     |  0.667  |  0.421    |  0.547\n6     |  0.101     |  0.573  |  0.172    |  0.597\n7     |  0.202     |  0.59  |  0.301    |  0.746\n8     |  0.187     |  0.364  |  0.247    |  0.84\n9     |  0.221     |  0.236  |  0.228    |  0.921\n10     |  0.333     |  0.017  |  0.032    |  0.9\n\naccuracy :     0.507\n\n\n Modèle n°10 : FINISHED \n\n\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"b9f6acf0540840dd933cff7a400309ce","source_hash":null,"execution_start":1677507446775,"execution_millis":4,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"round(class_accuracy.mean() * 100, 1)","block_group":"b9f6acf0540840dd933cff7a400309ce","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"0 acc_0     90.8\n0 acc_1     90.1\n1 acc_0     94.6\n1 acc_1     87.7\n2 acc_0     85.6\n2 acc_1     71.5\n3 acc_0     77.9\n3 acc_1     55.3\n4 acc_0     76.1\n4 acc_1     43.7\n5 acc_0     73.3\n5 acc_1     55.6\n6 acc_0     84.7\n6 acc_1     59.2\n7 acc_0     86.8\n7 acc_1     75.1\n8 acc_0     89.5\n8 acc_1     84.7\n9 acc_0     95.1\n9 acc_1     92.7\n10 acc_0    91.2\n10 acc_1    91.1\ndtype: float64"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"20fe37db8ad44d039ea1395671fcfdd8","source_hash":null,"execution_start":1677138575549,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"models_df = pd.DataFrame({'basic':[ 22, 22, 22, 22, 23, 22, 20, 23, 24, 20],\n        'opti': [ 73.4, 73.4, 73.4, 73.5, 73.5, 73.3, 73.2, 73.4, 73.6, 73.3]} )\nprint(\"basc: \",models_df.basic.mean(),\"\\nopti: \",models_df.opti.mean())","block_group":"20fe37db8ad44d039ea1395671fcfdd8","execution_count":null,"outputs":[{"name":"stdout","text":"basc:  22.0 \nopti:  73.4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=df420847-9b7d-4085-bb2b-2d13083511fa' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"a1b7501b32a149528d4f832ee492752e","deepnote_persisted_session":{"createdAt":"2023-02-27T15:16:55.825Z"},"deepnote_execution_queue":[]}}